{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from st_diff_viewer import diff_viewer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "from PyPDF2 import PdfReader\n",
    "import time\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "import tiktoken\n",
    "import openai\n",
    "\n",
    "# Configuration\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=400,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "def num_tokens(text):\n",
    "    \"\"\"Count tokens using tiktoken\"\"\"\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "# Tokenizer for GPT-4\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "# Custom theme and session state initialization\n",
    "st.set_page_config(layout=\"wide\")\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    [data-testid=stSidebar] {\n",
    "        background-color: #f0f2f6;\n",
    "    }\n",
    "    .stProgress > div > div > div > div {\n",
    "        background-color: #4B8BF5;\n",
    "    }\n",
    "    .st-b7 {\n",
    "        color: #262730;\n",
    "    }\n",
    "    .report-section { \n",
    "        border-left: 4px solid #4B8BF5;\n",
    "        padding-left: 1rem;\n",
    "        margin: 1rem 0;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Retry configuration for rate limits\n",
    "def retriable_chain_invoke(chain, inputs):\n",
    "    return chain.invoke(inputs)\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    retry=retry_if_exception_type(openai.RateLimitError),\n",
    "    reraise=True\n",
    ")\n",
    "def retriable_chain_invoke(chain, inputs):\n",
    "    \"\"\"Wrapper with retry logic for OpenAI API calls\"\"\"\n",
    "    return chain.invoke(inputs)\n",
    "\n",
    "def num_tokens(text):\n",
    "    \"\"\"Calculate token count for text\"\"\"\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def extract_text(file):\n",
    "    \"\"\"Extract text from DOCX or PDF files\"\"\"\n",
    "    if file.name.endswith('.docx'):\n",
    "        return docx2txt.process(file)\n",
    "    elif file.name.endswith('.pdf'):\n",
    "        reader = PdfReader(file)\n",
    "        return \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "    raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "def process_document(file, doc_type, year):\n",
    "    \"\"\"Process document into chunks with metadata\"\"\"\n",
    "    text = extract_text(file)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return [{\n",
    "        \"text\": chunk,\n",
    "        \"metadata\": {\n",
    "            \"source\": file.name,\n",
    "            \"doc_type\": doc_type,\n",
    "            \"year\": year,\n",
    "            \"page\": (idx // 10) + 1\n",
    "        }\n",
    "    } for idx, chunk in enumerate(chunks)]\n",
    "\n",
    "def analyze_changes(doc1_chunks, doc2_chunks):\n",
    "    \"\"\"Analyze documents using FAISS embeddings with rate limit handling\"\"\"\n",
    "    embeddings = OpenAIEmbeddings(chunk_size=10)  # Smaller chunk size for rate limiting\n",
    "    \n",
    "    doc1_docs = [Document(page_content=c[\"text\"], metadata=c[\"metadata\"]) \n",
    "                for c in doc1_chunks]\n",
    "    doc2_docs = [Document(page_content=c[\"text\"], metadata=c[\"metadata\"]) \n",
    "                for c in doc2_chunks]\n",
    "\n",
    "    # Create vector stores with error handling\n",
    "    db1 = FAISS.from_documents(doc1_docs, embeddings)\n",
    "    time.sleep(1)  # Rate limit buffer\n",
    "    db2 = FAISS.from_documents(doc2_docs, embeddings)\n",
    "\n",
    "    changes = {\"added\": [], \"removed\": [], \"modified\": []}\n",
    "\n",
    "    # Batch processing for rate limiting\n",
    "    batch_size = 5\n",
    "    for i in range(0, len(doc2_docs), batch_size):\n",
    "        batch = doc2_docs[i:i+batch_size]\n",
    "        for doc in batch:\n",
    "            similar = db1.similarity_search(doc.page_content, k=1)\n",
    "            if not similar or similar[0].metadata[\"source\"] != doc.metadata[\"source\"]:\n",
    "                changes[\"added\"].append({\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata\n",
    "                })\n",
    "        time.sleep(1)\n",
    "\n",
    "    for i in range(0, len(doc1_docs), batch_size):\n",
    "        batch = doc1_docs[i:i+batch_size]\n",
    "        for doc in batch:\n",
    "            similar = db2.similarity_search(doc.page_content, k=1)\n",
    "            if not similar or similar[0].metadata[\"source\"] != doc.metadata[\"source\"]:\n",
    "                changes[\"removed\"].append({\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata\n",
    "                })\n",
    "        time.sleep(1)\n",
    "\n",
    "    for i in range(0, len(doc2_docs), batch_size):\n",
    "        batch = doc2_docs[i:i+batch_size]\n",
    "        for doc in batch:\n",
    "            similar = db1.similarity_search(doc.page_content, k=1)\n",
    "            if similar and similar[0].metadata[\"source\"] == doc.metadata[\"source\"]:\n",
    "                doc1 = similar[0]\n",
    "                if doc1.page_content != doc.page_content:\n",
    "                    changes[\"modified\"].append({\n",
    "                        \"original\": doc1.page_content,\n",
    "                        \"updated\": doc.page_content,\n",
    "                        \"metadata\": doc.metadata\n",
    "                    })\n",
    "        time.sleep(1)\n",
    "\n",
    "    return changes\n",
    "\n",
    "def generate_executive_summary(changes, doc1_meta, doc2_meta):\n",
    "    \"\"\"Generate human-readable summary with hierarchical processing\"\"\"\n",
    "    # 1. Cluster related changes using embeddings\n",
    "    cluster_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Cluster these document changes into logical groups based on semantic similarity:\n",
    "    {changes}\n",
    "    \n",
    "    Return ONLY a JSON array of cluster objects with:\n",
    "    - \"theme\": Short descriptive title\n",
    "    - \"change_ids\": Array of original change indices\n",
    "    - \"key_phrases\": 3-5 key phrases per cluster\n",
    "    \"\"\")\n",
    "    \n",
    "    # 2. Hierarchical summarization chain\n",
    "    summary_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    As a professional analyst, create an executive summary comparing:\n",
    "    {doc1} ({year1}) vs {doc2} ({year2}).\n",
    "    \n",
    "    Key clustered changes:\n",
    "    {clustered_changes}\n",
    "\n",
    "    Structure:\n",
    "    1. **Major Structural Changes** (sections added/removed)\n",
    "    2. **Content Evolution** (modified themes and concepts)\n",
    "    3. **Strategic Implications** (business impact analysis)\n",
    "    4. **Recommendations** (next steps based on changes)\n",
    "    \n",
    "    Include specific examples with citations like: \n",
    "    \"The address changed from [X][p3] to [Y][p12]\" \n",
    "    Use markdown with section headers and bold key terms.\n",
    "    \"\"\")\n",
    "\n",
    "    # Implementation steps\n",
    "    cluster_chain = cluster_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0.1)\n",
    "    summary_chain = summary_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0.3)\n",
    "\n",
    "    # Process in batches using research-backed methods [1][3][5]\n",
    "    def chunk_changes(changes, max_tokens=6000):\n",
    "        current_chunk = []\n",
    "        current_count = 0\n",
    "        for idx, item in enumerate(changes):\n",
    "            item_tokens = num_tokens(item['content'])\n",
    "            if current_count + item_tokens > max_tokens:\n",
    "                yield current_chunk\n",
    "                current_chunk = []\n",
    "                current_count = 0\n",
    "            current_chunk.append((idx, item))\n",
    "            current_count += item_tokens\n",
    "        if current_chunk:\n",
    "            yield current_chunk\n",
    "\n",
    "    # Cluster changes using hierarchical approach [8]\n",
    "    all_clusters = []\n",
    "    for chunk in chunk_changes(\n",
    "        [c for cat in changes.values() for c in cat], \n",
    "        max_tokens=6000\n",
    "    ):\n",
    "        cluster_result = retriable_chain_invoke(cluster_chain, {\n",
    "            \"changes\": \"\\n\".join(\n",
    "                f\"{idx}: {item['content'][:500]}...\" \n",
    "                for idx, item in chunk\n",
    "            )\n",
    "        })\n",
    "        all_clusters.extend(json.loads(cluster_result.content))\n",
    "    \n",
    "    # Process clusters with context-aware summarization [1][4][6]\n",
    "    cluster_summaries = []\n",
    "    for cluster in all_clusters:\n",
    "        cluster_changes = [changes[i] for i in cluster[\"change_ids\"]]\n",
    "        cluster_text = \"\\n\".join(\n",
    "            f\"Change {i}: {c['content'][:1000]} [Source: {c['metadata']['source']}, Page {c['metadata']['page']}]\"\n",
    "            for i, c in zip(cluster[\"change_ids\"], cluster_changes)\n",
    "        )\n",
    "        \n",
    "        cluster_summary = retriable_chain_invoke(summary_chain, {\n",
    "            \"doc1\": doc1_meta[\"name\"],\n",
    "            \"year1\": doc1_meta[\"year\"],\n",
    "            \"doc2\": doc2_meta[\"name\"],\n",
    "            \"year2\": doc2_meta[\"year\"],\n",
    "            \"clustered_changes\": cluster_text\n",
    "        })\n",
    "        cluster_summaries.append(cluster_summary.content)\n",
    "        time.sleep(1)  # Rate limit buffer\n",
    "\n",
    "    # Final consolidation with cross-cluster analysis [8]\n",
    "    final_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Synthesize these cluster summaries into an executive report:\n",
    "    {cluster_summaries}\n",
    "\n",
    "    Maintain this structure:\n",
    "    1. **Document Evolution Overview**\n",
    "    2. **Strategic Direction Analysis**\n",
    "    3. **Operational Impact Assessment**\n",
    "    4. **Recommendations for Future Versions**\n",
    "\n",
    "    Include 3-5 key visualizable trends using **bold** terms.\n",
    "    Cite sources like [Source: {source}, Page {page}].\n",
    "    \"\"\")\n",
    "\n",
    "    final_chain = final_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "    final_summary = retriable_chain_invoke(final_chain, {\n",
    "        \"cluster_summaries\": \"\\n\\n\".join(cluster_summaries)\n",
    "    })\n",
    "    return final_summary.content, all_clusters\n",
    "\n",
    "def chunked_report_generator(changes, max_tokens=12000):\n",
    "    \"\"\"Split changes into token-sized chunks for GPT processing\"\"\"\n",
    "    current_chunk = []\n",
    "    current_count = 0\n",
    "    \n",
    "    for cat in ['added', 'removed', 'modified']:\n",
    "        for item in changes[cat]:\n",
    "            content = f\"{cat.upper()}:\\n{item['content']}\\n\"\n",
    "            tokens = num_tokens(content)\n",
    "            \n",
    "            if current_count + tokens > max_tokens:\n",
    "                yield current_chunk\n",
    "                current_chunk = []\n",
    "                current_count = 0\n",
    "                \n",
    "            current_chunk.append(content)\n",
    "            current_count += tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        yield current_chunk\n",
    "\n",
    "def generate_detailed_report(changes, doc1_meta, doc2_meta):\n",
    "    \"\"\"Generate detailed report with chunked processing\"\"\"\n",
    "    report_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Analyze document changes between:\n",
    "    {doc1} ({year1}) and {doc2} ({year2})\n",
    "\n",
    "    Changes:\n",
    "    {changes}\n",
    "\n",
    "    Format requirements:\n",
    "    - Group by ADDED/REMOVED/MODIFIED\n",
    "    - Include citations like [Source: {source}, Page {page}]\n",
    "    - Highlight significant changes with **bold**\n",
    "    - Use markdown headers ##\n",
    "    - Maintain academic tone\n",
    "    \"\"\")\n",
    "    \n",
    "    full_report = []\n",
    "    chain = report_prompt | ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=4000\n",
    "    )\n",
    "    \n",
    "    for chunk in chunked_report_generator(changes):\n",
    "        part = retriable_chain_invoke(chain, {\n",
    "            \"doc1\": doc1_meta[\"name\"],\n",
    "            \"year1\": doc1_meta[\"year\"],\n",
    "            \"doc2\": doc2_meta[\"name\"],\n",
    "            \"year2\": doc2_meta[\"year\"],\n",
    "            \"changes\": \"\\n\".join(chunk)\n",
    "        }).content\n",
    "        full_report.append(part)\n",
    "        time.sleep(2)  # Rate limit buffer\n",
    "    \n",
    "    return \"\\n\\n\".join(full_report)\n",
    "\n",
    "def main():\n",
    "    st.title(\"Professional Document Comparison Suite\")\n",
    "    \n",
    "    # Session state initialization\n",
    "    if 'colors' not in st.session_state:\n",
    "        st.session_state.colors = {\n",
    "            'added': '#d4f7d4',\n",
    "            'removed': '#f7d4d4',\n",
    "            'modified': '#fff3d4'\n",
    "        }\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configuration\")\n",
    "        st.session_state.colors['added'] = st.color_picker(\"Added Color\", '#d4f7d4')\n",
    "        st.session_state.colors['removed'] = st.color_picker(\"Removed Color\", '#f7d4d4')\n",
    "        st.session_state.colors['modified'] = st.color_picker(\"Modified Color\", '#fff3d4')\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        doc1 = st.file_uploader(\"Upload Baseline Document\", type=[\"pdf\", \"docx\"])\n",
    "        year1 = st.number_input(\"Baseline Year\", min_value=1900, max_value=2025, value=2023)\n",
    "    with col2:\n",
    "        doc2 = st.file_uploader(\"Upload Comparison Document\", type=[\"pdf\", \"docx\"])\n",
    "        year2 = st.number_input(\"Comparison Year\", min_value=1900, max_value=2025, value=2024)\n",
    "\n",
    "    if st.button(\"Analyze Documents\") and doc1 and doc2:\n",
    "        with st.status(\"Processing documents...\", expanded=True) as status:\n",
    "            try:\n",
    "                # Process documents\n",
    "                st.write(\"üìÑ Processing Document 1...\")\n",
    "                doc1_chunks = process_document(doc1, doc1.name.split('.')[-1], year1)\n",
    "                \n",
    "                st.write(\"üìÑ Processing Document 2...\")\n",
    "                doc2_chunks = process_document(doc2, doc2.name.split('.')[-1], year2)\n",
    "                \n",
    "                # Analyze changes\n",
    "                st.write(\"üîç Analyzing differences...\")\n",
    "                changes = analyze_changes(doc1_chunks, doc2_chunks)\n",
    "                \n",
    "                # Generate reports\n",
    "                st.write(\"üìä Generating summary...\")\n",
    "                summary, all_clusters = generate_executive_summary(  # Unpack tuple\n",
    "                    changes,\n",
    "                    {\"name\": doc1.name, \"year\": year1},\n",
    "                    {\"name\": doc2.name, \"year\": year2}\n",
    ")\n",
    "                \n",
    "                st.write(\"üìù Compiling detailed report...\")\n",
    "                detailed_report = generate_detailed_report(\n",
    "                    changes,\n",
    "                    {\"name\": doc1.name, \"year\": year1},\n",
    "                    {\"name\": doc2.name, \"year\": year2}\n",
    "                )\n",
    "                \n",
    "                # Combine reports\n",
    "                full_report = f\"# Document Comparison Report\\n{summary}\\n{detailed_report}\"\n",
    "                \n",
    "                status.update(label=\"Analysis complete! ‚úÖ\", state=\"complete\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                st.error(f\"‚ùå Processing failed: {str(e)}\")\n",
    "                return\n",
    "\n",
    "        # Visualization\n",
    "        with st.expander(\"Detailed Analysis\", expanded=True):\n",
    "            tab1, tab2, tab3, tab4 = st.tabs([\"Diff View\", \"Statistics\", \"Full Report\", \"Executive Summary\"])\n",
    "            \n",
    "            with tab1:\n",
    "                diff_viewer(\n",
    "                    \"\\n\".join([c[\"content\"] for c in changes[\"removed\"]]),\n",
    "                    \"\\n\".join([c[\"content\"] for c in changes[\"added\"]]),\n",
    "                    split_view=True,\n",
    "                    added_style=f\"background: {st.session_state.colors['added']}\",\n",
    "                    removed_style=f\"background: {st.session_state.colors['removed']}\",\n",
    "                    modified_style=f\"background: {st.session_state.colors['modified']}\"\n",
    "                )\n",
    "            \n",
    "            with tab2:\n",
    "                stats = pd.DataFrame({\n",
    "                    'Change Type': ['Added', 'Removed', 'Modified'],\n",
    "                    'Count': [\n",
    "                        len(changes[\"added\"]), \n",
    "                        len(changes[\"removed\"]), \n",
    "                        len(changes[\"modified\"])\n",
    "                    ]\n",
    "                })\n",
    "                st.dataframe(\n",
    "                    stats.style.applymap(\n",
    "                        lambda x: f\"background-color: {st.session_state.colors[x.lower()]};\", \n",
    "                        subset=['Change Type']\n",
    "                    ),\n",
    "                    use_container_width=True\n",
    "                )\n",
    "            \n",
    "            with tab3:\n",
    "                st.markdown(full_report, unsafe_allow_html=True)\n",
    "            \n",
    "            with tab4:\n",
    "                st.markdown(summary, unsafe_allow_html=True)\n",
    "                st.write(\"### Change Clusters\")\n",
    "                \n",
    "                if all_clusters:\n",
    "                    for cluster in all_clusters:\n",
    "                        with st.expander(f\"{cluster.get('theme', 'Unnamed Cluster')}\"):\n",
    "                            st.write(f\"**Key Phrases**: {', '.join(cluster.get('key_phrases', []))}\")\n",
    "                            st.write(f\"Associated Changes: {len(cluster.get('change_ids', []))}\")\n",
    "                            st.write(f\"Example Change: {changes[cluster['change_ids'][0]]['content'][:200]}...\")\n",
    "                else:\n",
    "                    st.warning(\"No clusters identified in document changes\")\n",
    "        \n",
    "        # Download button\n",
    "        st.download_button(\n",
    "            label=\"üì• Download Full Report\",\n",
    "            data=full_report,\n",
    "            file_name=\"document_comparison.md\",\n",
    "            mime=\"text/markdown\"\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
